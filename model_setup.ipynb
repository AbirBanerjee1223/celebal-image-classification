{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ffcefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Import all necessary packages ---\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "# Image Processing & Feature Extraction\n",
    "from skimage.feature import hog\n",
    "\n",
    "# Modeling, evaluation, and hyperparameter tuning\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Metrics and Reporting\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7fc008",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    # --- Paths for Colab environment ---\n",
    "    \"dataset_path\": \"caltech_15_classes\", # Using the new 15-class dataset\n",
    "    \"output_artifacts_path\": \"image_classifier_artifacts_v1.joblib\",\n",
    "\n",
    "    # --- Preprocessing (Optimized for speed and accuracy) ---\n",
    "    \"image_size\": (128, 128), # Smaller size for faster feature extraction\n",
    "\n",
    "    # --- Feature Extraction (HOG-only for the strongest signal) ---\n",
    "    \"features\": {\n",
    "        \"hog_orientations\": 9,\n",
    "        \"hog_pixels_per_cell\": (16, 16), # Larger cells for a more compact feature vector\n",
    "        \"hog_cells_per_block\": (2, 2),\n",
    "    },\n",
    "    \n",
    "    # --- Model Development ---\n",
    "    \"training\": {\n",
    "        \"test_size\": 0.25,\n",
    "        \"random_state\": 42,\n",
    "        \"tuning_n_iter\": 10,  # 10 iterations is a good balance for speed\n",
    "        \"tuning_cv\": 3,       # 3-fold cross-validation\n",
    "    }\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81287f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 3: FEATURE EXTRACTION PIPELINE\n",
    "# ==============================================================================\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    \"\"\"Handles Image Loading, Validation, and Resizing.\"\"\"\n",
    "    try:\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None: return None\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, CONFIG[\"image_size\"])\n",
    "        return image\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_hog_features(image):\n",
    "    \"\"\"Extracts Histogram of Oriented Gradients (HOG) features.\"\"\"\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    features = hog(gray_image,\n",
    "                   orientations=CONFIG[\"features\"][\"hog_orientations\"],\n",
    "                   pixels_per_cell=CONFIG[\"features\"][\"hog_pixels_per_cell\"],\n",
    "                   cells_per_block=CONFIG[\"features\"][\"hog_cells_per_block\"],\n",
    "                   transform_sqrt=True, block_norm='L2-Hys')\n",
    "    return features\n",
    "\n",
    "print(\"‚úÖ Optimized feature extraction pipeline defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125a47e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 4: MAIN EXECUTION - DATA LOADING, TRAINING, AND SAVING\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Load Data & Extract HOG Features ---\n",
    "print(\"\\n--- Starting Data Processing ---\")\n",
    "features_list = []\n",
    "labels_list = []\n",
    "class_names = sorted(os.listdir(CONFIG[\"dataset_path\"]))\n",
    "\n",
    "for class_name in tqdm(class_names, desc=\"Extracting HOG Features\"):\n",
    "    class_path = os.path.join(CONFIG[\"dataset_path\"], class_name)\n",
    "    if not os.path.isdir(class_path): continue\n",
    "    \n",
    "    for image_name in os.listdir(class_path):\n",
    "        image_path = os.path.join(class_path, image_name)\n",
    "        processed_image = preprocess_image(image_path)\n",
    "        if processed_image is None: continue\n",
    "        \n",
    "        features = extract_hog_features(processed_image)\n",
    "        features_list.append(features)\n",
    "        labels_list.append(class_name)\n",
    "\n",
    "# --- Prepare Data for Modeling ---\n",
    "X = np.array(features_list)\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(labels_list)\n",
    "print(f\"\\nExtracted {X.shape[1]} HOG features for {len(X)} images.\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=CONFIG[\"training\"][\"test_size\"],\n",
    "    random_state=CONFIG[\"training\"][\"random_state\"], stratify=y\n",
    ")\n",
    "\n",
    "# --- Scale Features ---\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(\"Features have been scaled.\")\n",
    "\n",
    "# --- Model Training & Tuning ---\n",
    "print(\"\\n--- Starting Model Training & Hyperparameter Tuning ---\")\n",
    "\n",
    "models = {\n",
    "    \"SVM\": SVC(probability=True, random_state=CONFIG[\"training\"][\"random_state\"]),\n",
    "    \"RandomForest\": RandomForestClassifier(random_state=CONFIG[\"training\"][\"random_state\"], n_jobs=-1),\n",
    "    \"LightGBM\": lgb.LGBMClassifier(random_state=CONFIG[\"training\"][\"random_state\"], n_jobs=-1)\n",
    "}\n",
    "\n",
    "param_spaces = {\n",
    "    \"SVM\": {'C': [1, 10, 100], 'kernel': ['rbf'], 'gamma': [0.001, 0.01]},\n",
    "    \"RandomForest\": {'n_estimators': [100, 200], 'max_depth': [20, None], 'min_samples_split': [2, 5]},\n",
    "    \"LightGBM\": {'n_estimators': [100, 200], 'learning_rate': [0.1, 0.2], 'num_leaves': [31, 50]}\n",
    "}\n",
    "\n",
    "best_estimators = {}\n",
    "model_iterator = tqdm(models.items(), desc=\"Overall Model Tuning\", total=len(models))\n",
    "\n",
    "for name, model in model_iterator:\n",
    "    model_iterator.set_description(f\"Tuning {name}\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"‚ñ∂Ô∏è  [{datetime.now().strftime('%H:%M:%S')}] Starting RandomizedSearch for {name}...\")\n",
    "    \n",
    "    rs = RandomizedSearchCV(\n",
    "        model, param_spaces[name],\n",
    "        n_iter=CONFIG[\"training\"][\"tuning_n_iter\"],\n",
    "        cv=CONFIG[\"training\"][\"tuning_cv\"],\n",
    "        n_jobs=-1,\n",
    "        random_state=CONFIG[\"training\"][\"random_state\"],\n",
    "        verbose=1\n",
    "    ).fit(X_train_scaled, y_train)\n",
    "    \n",
    "    print(f\"‚úÖ [{datetime.now().strftime('%H:%M:%S')}] Finished tuning for {name}.\")\n",
    "    best_estimators[name] = rs.best_estimator_\n",
    "\n",
    "# --- Evaluation & Saving ---\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"üèÅ [{datetime.now().strftime('%H:%M:%S')}] All models tuned. Evaluating test set performance...\")\n",
    "\n",
    "results = []\n",
    "for name, model in best_estimators.items():\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    results.append({\"Model\": name, \"Test Accuracy\": accuracy})\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(by=\"Test Accuracy\", ascending=False)\n",
    "print(\"\\n--- Final Model Performance ---\")\n",
    "print(results_df)\n",
    "\n",
    "best_model_name = results_df.iloc[0][\"Model\"]\n",
    "best_model = best_estimators[best_model_name]\n",
    "print(f\"\\nüèÜ Best Performing Model: {best_model_name}\")\n",
    "\n",
    "# --- Save Final Artifacts ---\n",
    "artifacts = {\n",
    "    \"model\": best_model,\n",
    "    \"label_encoder\": le,\n",
    "    \"scaler\": scaler,\n",
    "    \"config\": CONFIG\n",
    "}\n",
    "joblib.dump(artifacts, CONFIG[\"output_artifacts_path\"])\n",
    "print(f\"\\n‚úÖ All artifacts saved to your Google Drive at: {CONFIG['output_artifacts_path']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
